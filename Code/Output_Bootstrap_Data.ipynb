{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-presentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard data science libraries\n",
    "# Must run this cell first\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import math\n",
    "import copy as cp\n",
    "import time\n",
    "\n",
    "plt.close('all')\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.optimize import minimize, least_squares\n",
    "\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "\n",
    "import scipy.stats as st\n",
    "from scipy import signal\n",
    "import scipy\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.fftpack import fft, ifft, rfft, irfft\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import itertools\n",
    "from itertools import permutations\n",
    "\n",
    "import sys\n",
    "\n",
    "import csv\n",
    "\n",
    "import pyunicorn\n",
    "from pyunicorn import timeseries\n",
    "from pyunicorn.timeseries.surrogates import Surrogates\n",
    "\n",
    "from matplotlib import cm\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import random\n",
    "import ray\n",
    "import NeuronVasomotionFunc as nvf\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-kingdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import SU pickled data\n",
    "\n",
    "with open(\"bs_SU_results_sim_detrended_store_synch.txt\", \"rb\") as fp:   # Unpickling\n",
    "    raw_bs_data_SU = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-corruption",
   "metadata": {},
   "source": [
    "SU Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-preserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Organize data for actual SU data\n",
    "\n",
    "# Data is re-formatted to fit into individual cells\n",
    "total_num_iter = 1\n",
    "num_files = 43\n",
    "beg_ind = 0\n",
    "end_ind = 1200\n",
    "Tw = 240\n",
    "des_num_neurons = 25\n",
    "\n",
    "bs_sig_exp = np.zeros((total_num_iter,1))\n",
    "bs_score_mat = np.empty((total_num_iter, num_files, (end_ind-beg_ind) - Tw))\n",
    "bs_neuron_data_storage = np.zeros((total_num_iter, num_files, Tw, des_num_neurons))\n",
    "bs_PO2_data_storage = np.zeros((total_num_iter, num_files, Tw, des_num_neurons))\n",
    "bs_corr_mat = np.zeros((total_num_iter, num_files, des_num_neurons))\n",
    "bs_pvalue_mat = np.zeros((total_num_iter, num_files, des_num_neurons))\n",
    "bs_SI_mat = np.empty((total_num_iter, num_files, (end_ind-beg_ind) - Tw))\n",
    "\n",
    "bs_sig_exp[0,:] = raw_bs_data_SU[0][0]\n",
    "bs_score_mat[0,:,:] = raw_bs_data_SU[0][1]\n",
    "bs_neuron_data_storage[0,:,:,:] = raw_bs_data_SU[0][2]\n",
    "bs_PO2_data_storage[0,:,:,:] = raw_bs_data_SU[0][3]\n",
    "bs_corr_mat[0,:,:] = raw_bs_data_SU[0][4]\n",
    "bs_pvalue_mat[0,:,:] = raw_bs_data_SU[0][5]\n",
    "bs_SI_mat[0,:,:] = raw_bs_data_SU[0][9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-notion",
   "metadata": {},
   "outputs": [],
   "source": [
    "####This sub-script outputs two files and one folder of files ####\n",
    "\n",
    "### First file: Output synchronization index values for each experiment ###\n",
    "\n",
    "### Second file: Output summary table for actual data (For each row in the table): Experiment Name, \n",
    "### Number of Bonferonni Correlated Neurons, Correlation of each neuron, p-values for each correlation, \n",
    "### Time stamps of best fit time window, Number of neurons within experiment, \n",
    "### Bonferroni p-value threshold, Synchronization Index value, Percent of neurons in participating cluster\n",
    "### Neurons in the participating cluster, \n",
    "\n",
    "### First folder: Output a file for each experiment, consisting of PO2 and the corresponding Bonferroni \n",
    "### correlated neuron for the best fit two minute time window.\n",
    "\n",
    "# Indicator on whether to output data\n",
    "output_data = 1\n",
    "\n",
    "num_iter = bs_neuron_data_storage.shape[0]\n",
    "num_exp = bs_neuron_data_storage.shape[1]\n",
    "Tw_length = bs_neuron_data_storage.shape[2]\n",
    "num_neurons = bs_neuron_data_storage.shape[3]\n",
    "\n",
    "alpha = .05         # p-value significance level\n",
    "N = 100             # Number of surrogates to run for Synchronization Index analysis\n",
    "\n",
    "PI_thresh = .3      # Threshold of participation index in order to be added to participation cluster \n",
    "sync_thresh = 0     # Threshold of synchronization index to record experiment's synchronization index\n",
    "\n",
    "# Initialize variables\n",
    "SyncValues = []    \n",
    "sig_sync_counter = 0\n",
    "cluster_perc_list = []\n",
    "Synch_neuron_cluster_list = []\n",
    "\n",
    "for j in np.arange(num_exp):\n",
    "    exp_pvalue_vec = bs_pvalue_mat[0,j,:]\n",
    "\n",
    "    temp_num_neurons = np.sum(exp_pvalue_vec > 0)\n",
    "    exp_pvalue_vec = exp_pvalue_vec[:temp_num_neurons]\n",
    "\n",
    "    temp_bonf_alpha = .05/temp_num_neurons\n",
    "\n",
    "    if np.sum(exp_pvalue_vec < temp_bonf_alpha) > 0:\n",
    "        temp_X_data = bs_neuron_data_storage[0,j,:,:temp_num_neurons]\n",
    "\n",
    "        X_norm_temp_orig = (temp_X_data - np.mean(temp_X_data, axis = 0))/np.std(temp_X_data, axis = 0)\n",
    "\n",
    "        X_p = X_norm_temp_orig.T\n",
    "\n",
    "        cov_mat = np.cov(X_p)\n",
    "\n",
    "        eig_values, eig_vecs = np.linalg.eig(cov_mat)\n",
    "\n",
    "        sort_ind = np.argsort(eig_values)\n",
    "        sorted_eig_values = eig_values[sort_ind]\n",
    "        sorted_eig_vecs = eig_vecs[:,sort_ind]\n",
    "\n",
    "        smooth = [1,6]\n",
    "\n",
    "        sync_ind, PI_mat, _, _ = nvf.CMA_Sync(X_norm_temp_orig, N, sorted_eig_values, \n",
    "                                     sorted_eig_vecs, temp_bonf_alpha, smooth)\n",
    "\n",
    "        PI_vec = PI_mat[0,:]\n",
    "        \n",
    "        perc_cluster = (np.sum(PI_vec > PI_thresh))/len(PI_vec)\n",
    "        cluster_perc_list.append(perc_cluster)\n",
    "        \n",
    "        neuron_ind_range = np.arange(1, len(PI_vec)+1)\n",
    "        synch_cluster_neurons = neuron_ind_range[PI_vec > PI_thresh]\n",
    "        \n",
    "        Synch_neuron_cluster_list.append(synch_cluster_neurons)\n",
    "        \n",
    "        sync_ind1_value = sync_ind[-1]\n",
    "\n",
    "        SyncValues.append(sync_ind1_value)\n",
    "\n",
    "        if sync_ind1_value > sync_thresh:\n",
    "            sig_sync_counter += 1\n",
    "\n",
    "if output_data:            \n",
    "    np.savetxt('[output_folder]/SU_Sync_Ind_Distr_Actual.csv', SyncValues, delimiter=',')\n",
    "\n",
    "## Output Bonferonni Significant Exp on Actual Data\n",
    "score_vec = raw_bs_data_SU[0][1]\n",
    "neuron_series_data = raw_bs_data_SU[0][2]\n",
    "PO2_series_data = raw_bs_data_SU[0][3]\n",
    "corr_data = raw_bs_data_SU[0][4]\n",
    "pvalue_data = raw_bs_data_SU[0][5]\n",
    "sig_exp_data = raw_bs_data_SU[0][6]\n",
    "\n",
    "exp_list = []\n",
    "sig_neuron_num_list = []\n",
    "corr_list = []\n",
    "pvalue_list = []\n",
    "score_vec_list = []\n",
    "time_stamps = []\n",
    "pop_neuron_list = []\n",
    "bonf_alpha_list = []\n",
    "neuron_series_list = []\n",
    "PO2_series_list = []\n",
    "\n",
    "for i in np.arange(corr_data.shape[1]):\n",
    "    \n",
    "    temp_full_pvalue = pvalue_data[0,i,:]\n",
    "    temp_full_corr = corr_data[0,i,:]\n",
    "    \n",
    "    num_actual = np.sum(temp_full_pvalue > 0) \n",
    "    temp_bonf_alpha = .05/num_actual\n",
    "    \n",
    "    temp_pvalue_vec = temp_full_pvalue[:num_actual]\n",
    "    temp_corr_vec = temp_full_corr[:num_actual]\n",
    "    \n",
    "    sig_log = temp_pvalue_vec < temp_bonf_alpha\n",
    "    \n",
    "    if np.sum(sig_log) > 0:\n",
    "        \n",
    "        neuron_indices = np.arange(1, num_actual+1)\n",
    "        sig_neurons = neuron_indices[sig_log]\n",
    "        \n",
    "        exp_score_vec = score_vec[0, i, :]\n",
    "        \n",
    "        max_ind = np.argmax(exp_score_vec)\n",
    "        \n",
    "        beg_time_stamp = max_ind/2\n",
    "        end_time_stamp = (max_ind + Tw)/2\n",
    "        \n",
    "        exp_list.append(sig_exp_data[i])\n",
    "        sig_neuron_num_list.append(list(sig_neurons))\n",
    "        corr_list.append(temp_corr_vec[sig_log])\n",
    "        pvalue_list.append(temp_pvalue_vec[sig_log])\n",
    "        score_vec_list.append(exp_score_vec)\n",
    "        time_stamps.append(list([beg_time_stamp, end_time_stamp]))\n",
    "        pop_neuron_list.append(num_actual)\n",
    "        bonf_alpha_list.append(temp_bonf_alpha)\n",
    "        neuron_series_list.append(neuron_series_data[0,i,:,:])\n",
    "        PO2_series_list.append(PO2_series_data[0,i,:,:])\n",
    "\n",
    "if output_data:\n",
    "\n",
    "    output_path = '[Insert output path for actual data summary]'\n",
    "\n",
    "    import csv\n",
    "\n",
    "    with open(output_path,'w') as result_file:\n",
    "        wr = csv.writer(result_file, dialect='excel')\n",
    "\n",
    "        header = ['Experiment', 'Neuron Num(s)', 'Correlation(s)', 'P-value(s)', 'Time Stamps', \n",
    "                  'Pop Neuron Size', 'Bonf Alpha', 'Sync Ind', 'Perc Synch', 'Synchronized Neurons']\n",
    "        wr.writerow(header)\n",
    "\n",
    "        for i in np.arange(len(exp_list)):\n",
    "            temp_time_stamps = str(time_stamps[i][0]) + 's-' + str(time_stamps[i][1]) + 's'\n",
    "            wr.writerow([exp_list[i], sig_neuron_num_list[i], corr_list[i], pvalue_list[i],\n",
    "                         temp_time_stamps, pop_neuron_list[i], bonf_alpha_list[i], SyncValues[i],\n",
    "                         cluster_perc_list[i], Synch_neuron_cluster_list[i]])\n",
    "    \n",
    "    example_folder_path = '[Insert output path for best examples]'\n",
    "\n",
    "    if not os.path.isdir(example_folder_path):\n",
    "        os.mkdir(example_folder_path)\n",
    "        \n",
    "    for j in np.arange(len(exp_list)):\n",
    "        \n",
    "        temp_exp_name = exp_list[j]\n",
    "        \n",
    "        example_output_path = example_folder_path + temp_exp_name + '.csv'\n",
    "        \n",
    "        temp_sig_neuron_inds = np.array(sig_neuron_num_list[j])-1\n",
    "        \n",
    "        temp_SU_mat = neuron_series_list[j][:, temp_sig_neuron_inds]\n",
    "        temp_PO2_mat = PO2_series_list[j][:, 0]\n",
    "        \n",
    "        temp_PO2_mat = temp_PO2_mat.reshape(-1,1)\n",
    "        \n",
    "        norm_SU_mat = (temp_SU_mat - np.mean(temp_SU_mat, axis = 0))/np.std(temp_SU_mat, axis = 0)\n",
    "        norm_PO2_mat = (temp_PO2_mat - np.mean(temp_PO2_mat, axis = 0))/np.std(temp_PO2_mat, axis = 0)\n",
    "        \n",
    "        temp_output_mat = np.concatenate((norm_PO2_mat, norm_SU_mat), axis=1)\n",
    "        \n",
    "        with open(example_output_path,'w') as result_file:\n",
    "            wr = csv.writer(result_file, dialect='excel')\n",
    "\n",
    "            header = []\n",
    "            header.append(\"PO2\")\n",
    "            \n",
    "            num_sig = len(temp_sig_neuron_inds)\n",
    "\n",
    "            for k in np.arange(len(temp_sig_neuron_inds)):\n",
    "                header.append(\"SU\" + str(temp_sig_neuron_inds[k] + 1))\n",
    "            \n",
    "            wr.writerow(header)\n",
    "\n",
    "            for l in np.arange(temp_output_mat.shape[0]):\n",
    "                \n",
    "                wr.writerow(temp_output_mat[l,:])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-pound",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Interval hypothesis testing for synchronization ###\n",
    "\n",
    "# Determine distribution of synchronization index values\n",
    "actual_SI_distr = bs_SI_mat.flatten()\n",
    "\n",
    "# Create histogram\n",
    "dt = .01\n",
    "hist_bins = np.arange(0,1 + dt,dt)\n",
    "x = hist_bins[:-1]\n",
    "SI_hist = [np.sum((actual_SI_distr < hist_bins[i+1]) & (actual_SI_distr >= hist_bins[i])) \n",
    "           for i in np.arange(len(x))]\n",
    "\n",
    "SI_hist[0] = 0\n",
    "norm_SI_hist = SI_hist/np.sum(SI_hist)*(1/dt)\n",
    "\n",
    "sns.distplot(actual_SI_distr)\n",
    "plt.plot(x, norm_SI_hist)\n",
    "\n",
    "param_dt = .1\n",
    "beg_ind = 0\n",
    "end_ind = 30\n",
    "alpha_list = np.arange(beg_ind+param_dt, end_ind+param_dt, param_dt)\n",
    "beta_list = np.arange(beg_ind+param_dt, end_ind+param_dt, param_dt)\n",
    "\n",
    "MSE_mat = np.zeros((len(alpha_list), len(beta_list)))\n",
    "\n",
    "for i, alpha in enumerate(alpha_list):\n",
    "    print(i)\n",
    "    for j, beta in enumerate(beta_list):\n",
    "        temp_beta_distr = st.beta.pdf(x, alpha, beta)\n",
    "        \n",
    "        temp_MSE = np.mean(np.power(np.ma.masked_invalid((norm_SI_hist - temp_beta_distr)), 2))\n",
    "        \n",
    "        MSE_mat[i,j] = temp_MSE\n",
    "        \n",
    "alpha_inds = np.argmin(MSE_mat, axis=0)\n",
    "alpha_min = np.min(MSE_mat, axis=0)\n",
    "\n",
    "beta_ind = np.argmin(alpha_min)\n",
    "\n",
    "best_fit_param_ind = [alpha_inds[beta_ind], beta_ind]\n",
    "best_fit_param_value = [alpha_list[best_fit_param_ind[0]], beta_list[best_fit_param_ind[1]]]\n",
    "\n",
    "temp_alpha = best_fit_param_value[0]\n",
    "temp_beta = best_fit_param_value[1]\n",
    "\n",
    "theor_distr = st.beta.pdf(x, temp_alpha, temp_beta)\n",
    "\n",
    "output_mat = np.concatenate((x.reshape(-1,1), norm_SI_hist.reshape(-1,1), theor_distr.reshape(-1,1)), axis=1)\n",
    "\n",
    "output_path = '[output folder for synchroniztion index histogram]/SI_histogram.csv'\n",
    "\n",
    "with open(output_path, 'w') as result_file:\n",
    "    wr = csv.writer(result_file, dialect='excel')\n",
    "    \n",
    "    header = [\"Bin\", \"Empirical Distribution\", \"Theoretical Distribution\"]\n",
    "    \n",
    "    wr.writerow(header)\n",
    "    \n",
    "    for i in np.arange(output_mat.shape[0]):\n",
    "        temp_row = output_mat[i,:]\n",
    "        \n",
    "        wr.writerow(temp_row)\n",
    "\n",
    "plt.plot(x, theor_distr, color = 'r')\n",
    "plt.bar(x, norm_SI_hist, width=dt)\n",
    "\n",
    "1-st.beta.cdf(.1, temp_alpha, temp_beta)\n",
    "\n",
    "num_exp = bs_SI_mat.shape[1]\n",
    "\n",
    "total_exp_list = []\n",
    "max_synch_value_list = np.zeros((num_exp, 1))\n",
    "min_synch_value_list = np.zeros((num_exp, 1))\n",
    "max_synch_pvalue_list = np.zeros((num_exp, 1))\n",
    "min_synch_pvalue_list = np.zeros((num_exp, 1))\n",
    "\n",
    "print(len(exp_list))\n",
    "\n",
    "for i in np.arange(num_exp):\n",
    "    temp_exp = sig_exp_data[i]\n",
    "    total_exp_list.append(temp_exp)\n",
    "    \n",
    "    temp_score_vec = bs_score_mat[0,i,:]\n",
    "    temp_SI_vec = bs_SI_mat[0,i,:]\n",
    "    \n",
    "    score_max_ind = np.argmax(temp_score_vec)\n",
    "    max_synch_value = temp_SI_vec[score_max_ind]\n",
    "    \n",
    "    score_min_ind = np.argmin(temp_score_vec)\n",
    "    min_synch_value = temp_SI_vec[score_min_ind]\n",
    "    \n",
    "    max_synch_value_list[i] = max_synch_value\n",
    "    min_synch_value_list[i] = min_synch_value\n",
    "    \n",
    "    max_synch_pvalue = 1-st.beta.cdf(max_synch_value, temp_alpha, temp_beta)\n",
    "    min_synch_pvalue = 1-st.beta.cdf(min_synch_value, temp_alpha, temp_beta)\n",
    "    \n",
    "    max_synch_pvalue_list[i] = max_synch_pvalue\n",
    "    min_synch_pvalue_list[i] = min_synch_pvalue\n",
    "\n",
    "output_path = '[output folder for interval comparison test]/IntervalComparisonTesting_v2.csv'\n",
    "    \n",
    "with open(output_path,'w') as result_file:\n",
    "    wr = csv.writer(result_file, dialect='excel')\n",
    "    \n",
    "    header = [\"Experiment\", \"Max_TW_SI\", \"Min_TW_SI\", \"Max_TW_pvalue\", \"Min_TW_pvalue\"]\n",
    "    \n",
    "    wr.writerow(header)\n",
    "    \n",
    "    for i in np.arange(num_exp):\n",
    "        temp_exp = total_exp_list[i]\n",
    "        \n",
    "        if temp_exp in exp_list:\n",
    "            print(temp_exp)\n",
    "            temp_row = list((total_exp_list[i], max_synch_value_list[i][0], min_synch_value_list[i][0],\n",
    "                             max_synch_pvalue_list[i][0], min_synch_pvalue_list[i][0]))\n",
    "        \n",
    "            wr.writerow(temp_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-variable",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize simulated SU data\n",
    "\n",
    "num_cores = 12          # Number of cpu cores used to run analysis in \"Bootstrap Testing.ipynb\"\n",
    "base_ind_value  = 8     # Number of base iterations per core\n",
    "remainders = 4          # Number of remainders (i.e. number of cores which had base+1 iterations per core)\n",
    "des_num_var = 15        # Number of neurons per simulated experiment\n",
    "num_files = 43          # Number of experiments per iteration\n",
    "beg_ind = 0             # Starting end of experiment duration (.5 second timestep)\n",
    "end_ind = 1200          # Ending ind of experiment duration (.5 second timestep)\n",
    "Tw = 240                # Size of time window (.5 second timestep)\n",
    "\n",
    "iter_indices = np.ones((num_cores, 1))\n",
    "iter_indices = iter_indices*base_ind_value\n",
    "    \n",
    "for j in np.arange(remainders):\n",
    "    iter_indices[j] += 1\n",
    "\n",
    "iter_indices_cumsum = np.cumsum(iter_indices)\n",
    "\n",
    "total_num_iter = 100\n",
    "\n",
    "bs_sig_exp = np.zeros((total_num_iter,1))\n",
    "bs_score_mat = np.empty((total_num_iter, num_files, (end_ind-beg_ind) - Tw))\n",
    "bs_neuron_data_storage = np.zeros((total_num_iter, num_files, Tw, des_num_var))\n",
    "bs_PO2_data_storage = np.zeros((total_num_iter, num_files, Tw, des_num_var))\n",
    "bs_corr_mat = np.zeros((total_num_iter, num_files, des_num_var))\n",
    "bs_pvalue_mat = np.zeros((total_num_iter, num_files, des_num_var))\n",
    "bs_synch_score_mat = np.empty((total_num_iter, num_files, (end_ind-beg_ind) - Tw))\n",
    "\n",
    "for i in np.arange(len(iter_indices)):\n",
    "    if i == 0:\n",
    "        bot_ind = 0\n",
    "        top_ind = int(iter_indices_cumsum[i])\n",
    "    else:\n",
    "        bot_ind = int(iter_indices_cumsum[i-1])\n",
    "        top_ind = int(iter_indices_cumsum[i])\n",
    "    \n",
    "    bs_sig_exp[bot_ind:top_ind,:] = raw_bs_data_SU[i][0]\n",
    "    bs_score_mat[bot_ind:top_ind,:,:] = raw_bs_data_SU[i][1]\n",
    "    bs_neuron_data_storage[bot_ind:top_ind,:,:,:] = raw_bs_data_SU[i][2]\n",
    "    bs_PO2_data_storage[bot_ind:top_ind,:,:,:] = raw_bs_data_SU[i][3]\n",
    "    bs_corr_mat[bot_ind:top_ind,:,:] = raw_bs_data_SU[i][4]\n",
    "    bs_pvalue_mat[bot_ind:top_ind,:,:] = raw_bs_data_SU[i][5]\n",
    "    bs_synch_score_mat[bot_ind:top_ind,:,:] = raw_bs_data_SU[i][9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-animal",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Recalculate number of significant experiments for simulated data\n",
    "\n",
    "num_iter = bs_neuron_data_storage.shape[0]\n",
    "num_exp = bs_neuron_data_storage.shape[1]\n",
    "Tw_length = bs_neuron_data_storage.shape[2]\n",
    "num_neurons = bs_neuron_data_storage.shape[3]\n",
    "\n",
    "alpha = .05    # p-value significance level\n",
    "N = 100        # Number of surrogates to run for Synchronization Index analysis\n",
    "\n",
    "sig_exp_mat = np.zeros((num_iter,1))\n",
    "\n",
    "for i in np.arange(num_iter):\n",
    "\n",
    "    sig_exp_counter = 0\n",
    "    \n",
    "    for j in np.arange(num_exp):\n",
    "        exp_pvalue_vec = bs_pvalue_mat[i,j,:]\n",
    "        \n",
    "        temp_num_neurons = np.sum(exp_pvalue_vec > 0)\n",
    "        exp_pvalue_vec = exp_pvalue_vec[:temp_num_neurons]\n",
    "    \n",
    "        temp_bonf_alpha = .05/temp_num_neurons\n",
    "        \n",
    "        if np.sum(exp_pvalue_vec < temp_bonf_alpha) > 0:\n",
    "            sig_exp_counter += 1\n",
    "        \n",
    "    sig_exp_mat[i] = sig_exp_counter\n",
    "\n",
    "output_path = '/home/evan/Projects/NeuronVasomotion/StatSUResults/SU_Sim_Distr_NumBonf_detrend.csv'\n",
    "\n",
    "np.savetxt(output_path, sig_exp_mat, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-latino",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output best examples of SU surrogate data\n",
    "\n",
    "### First folder: Outputs 10 ten examples of simulated SU and PO2 times series over a two minute time window.\n",
    "### Examples are output in order from best to worst correlations (1st = best correlation, 10th = 10th best correlation)\n",
    "\n",
    "output_data = 0\n",
    "\n",
    "# Pull parameters\n",
    "num_iter = bs_corr_mat.shape[0]\n",
    "num_exp = bs_corr_mat.shape[1]\n",
    "num_SU = bs_corr_mat.shape[2]\n",
    "\n",
    "Tw_length = bs_neuron_data_storage.shape[2]\n",
    "\n",
    "# Set number of examples to generate\n",
    "num_examples = 10\n",
    "\n",
    "# Initialized matrices to find ts with best correlations\n",
    "best_corr_inds = np.zeros((num_iter, 2))\n",
    "best_corr_values = np.zeros(num_iter)\n",
    "\n",
    "# Find experiments and single units with best correlations\n",
    "for i in np.arange(num_iter):\n",
    "    best_iter_corr_inds = np.zeros(num_exp)\n",
    "    best_iter_corr_values = np.zeros(num_exp)\n",
    "\n",
    "    for j in np.arange(num_exp):\n",
    "\n",
    "        temp_corr_vec = bs_corr_mat[i,j,:]\n",
    "\n",
    "        best_iter_corr_inds[j] = np.nanargmax(temp_corr_vec) \n",
    "        best_iter_corr_values[j] = np.nanmax(temp_corr_vec)\n",
    "\n",
    "    best_exp_ind = np.argmax(best_iter_corr_values)\n",
    "    best_SU_ind = best_iter_corr_inds[best_exp_ind]\n",
    "\n",
    "    best_corr_inds[i,0] = best_exp_ind\n",
    "    best_corr_inds[i,1] = best_SU_ind\n",
    "\n",
    "    best_corr_values[i] = np.max(best_iter_corr_values)\n",
    "\n",
    "# Determine iterations with the highest correlations\n",
    "best_iter_list = np.argsort(best_corr_values)\n",
    "best_iter_list = best_iter_list[::-1]\n",
    "\n",
    "best_examples_mat = np.zeros((Tw_length, 2, num_examples))\n",
    "\n",
    "for k in np.arange(num_examples):\n",
    "    \n",
    "    best_iter = best_iter_list[k]    \n",
    "    best_corr_ind = [best_iter, int(best_corr_inds[best_iter,0]), int(best_corr_inds[best_iter,1])]\n",
    "\n",
    "    best_SU_series = bs_neuron_data_storage[best_corr_ind[0], best_corr_ind[1], :, best_corr_ind[2]]\n",
    "    best_PO2_series = bs_PO2_data_storage[best_corr_ind[0], best_corr_ind[1], :, best_corr_ind[2]]\n",
    "    \n",
    "    \n",
    "    best_examples_mat[:, 0, k] = best_PO2_series\n",
    "    best_examples_mat[:, 1, k] = best_SU_series\n",
    "\n",
    "plt.plot(best_examples_mat[:,0,1])\n",
    "plt.plot(best_examples_mat[:,1,1])\n",
    "\n",
    "if output_data:\n",
    "\n",
    "    for k in np.arange(num_examples):\n",
    "    \n",
    "        output_folder = '[output folder for best simulated examples]/Best_Simulated_SU_Examples_v2/'\n",
    "        \n",
    "        if not os.path.isdir(output_folder):\n",
    "            os.mkdir(output_folder)\n",
    "        \n",
    "        output_mat = best_examples_mat[:,:,k]\n",
    "\n",
    "        output_path = join(output_folder, 'best_SU_sim_example_detrend_case'+str(int(k+1))+'.csv')\n",
    "        \n",
    "        with open(output_path,'w') as result_file:\n",
    "            wr = csv.writer(result_file, dialect='excel')\n",
    "\n",
    "            wr.writerow(['PO2', \"SU\"])\n",
    "\n",
    "            for i in np.arange(output_mat.shape[0]):\n",
    "                temp_row = list((output_mat[i,0], output_mat[i,1]))\n",
    "\n",
    "                wr.writerow(temp_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-cholesterol",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Determine number of significant synchronizations for SU data\n",
    "\n",
    "num_iter = bs_neuron_data_storage.shape[0]\n",
    "num_exp = bs_neuron_data_storage.shape[1]\n",
    "Tw_length = bs_neuron_data_storage.shape[2]\n",
    "num_neurons = bs_neuron_data_storage.shape[3]\n",
    "\n",
    "alpha = .05      # p-value significance level \n",
    "N = 100          # Number of surrogates to run for Synchronization Index analysis\n",
    "\n",
    "sync_thresh = 0\n",
    "\n",
    "sig_sync_mat = np.zeros((num_iter,1))\n",
    "sync_data_mat = np.zeros((num_iter, num_exp))\n",
    "\n",
    "sync_values = []\n",
    "\n",
    "for i in np.arange(num_iter):\n",
    "    \n",
    "    print(i)\n",
    "    \n",
    "    sig_sync_counter = 0\n",
    "    \n",
    "    for j in np.arange(num_exp):\n",
    "        exp_pvalue_vec = bs_pvalue_mat[i,j,:]\n",
    "        #print(exp_pvalue_vec)\n",
    "        \n",
    "        temp_num_neurons = np.sum(exp_pvalue_vec > 0)\n",
    "        exp_pvalue_vec = exp_pvalue_vec[:temp_num_neurons]\n",
    "    \n",
    "        temp_bonf_alpha = .05/temp_num_neurons\n",
    "        #print(temp_bonf_alpha)\n",
    "        \n",
    "        if np.sum(exp_pvalue_vec < temp_bonf_alpha) > 0:\n",
    "            temp_X_data = bs_neuron_data_storage[i,j,:,:temp_num_neurons]\n",
    "            \n",
    "            #print(temp_X_data)\n",
    "            \n",
    "            X_norm_temp_orig = (temp_X_data - np.mean(temp_X_data, axis = 0))/np.std(temp_X_data, axis = 0)\n",
    "            \n",
    "            X_p = X_norm_temp_orig.T\n",
    "\n",
    "            cov_mat = np.cov(X_p)\n",
    "        \n",
    "            eig_values, eig_vecs = np.linalg.eig(cov_mat)\n",
    "\n",
    "            sort_ind = np.argsort(eig_values)\n",
    "            sorted_eig_values = eig_values[sort_ind]\n",
    "            sorted_eig_vecs = eig_vecs[:,sort_ind]\n",
    "\n",
    "            pca = X_p\n",
    "        \n",
    "            smooth = [1,6]\n",
    "        \n",
    "            sync_ind, _, _, _ = nvf.CMA_Sync(X_norm_temp_orig, N, sorted_eig_values, \n",
    "                                         sorted_eig_vecs, temp_bonf_alpha, smooth)\n",
    "            \n",
    "            sync_ind1_value = sync_ind[-1]\n",
    "            \n",
    "            sync_data_mat[i,j] = sync_ind1_value\n",
    "            \n",
    "            sync_values.append(sync_ind1_value)\n",
    "            \n",
    "            if sync_ind1_value > sync_thresh:\n",
    "                sig_sync_counter += 1\n",
    "    \n",
    "    sig_sync_mat[i] = sig_sync_counter\n",
    "    \n",
    "np.savetxt(\"/home/evan/Projects/NeuronVasomotion/StatSUResults/SU_Sim_Distr_Sync_Ind_detrend_v2.csv\", sync_values, delimiter=',')\n",
    "np.savetxt(\"/home/evan/Projects/NeuronVasomotion/StatSUResults/SU_Sim_Distr_NumSigExp_Sync_detrend_v2.csv\", sig_sync_mat, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-toddler",
   "metadata": {},
   "source": [
    "LFP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chinese-pattern",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"bs_LFP_results_actual_detrended.txt\", \"rb\") as fp:   # Unpickling\n",
    "    raw_bs_data_LFP = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-navigator",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Output summary table for actual data. Each row consists of the frequency range (band), \n",
    "### number of significant experiments, followed by all the significant experiments for the corresponding band.\n",
    "\n",
    "# Indicator whether to output LFP data\n",
    "output_data = 1\n",
    "\n",
    "band_ranges = ['0-4Hz', '4-7Hz', '8-15Hz', '16-31Hz', '30-50Hz', '50-80Hz', '80-100Hz']\n",
    "\n",
    "if output_data:\n",
    "    \n",
    "    output_path = '/home/evan/Projects/NeuronVasomotion/Output_LFP/LFP_band_actual_distr.csv'\n",
    "\n",
    "    with open(output_path,'w') as result_file:\n",
    "        wr = csv.writer(result_file, dialect='excel')\n",
    "\n",
    "        for i in np.arange(7):\n",
    "            temp_exp_list = raw_bs_data_LFP[i][8]\n",
    "            temp_num_sig_exp = raw_bs_data_LFP[i][0][0][0]\n",
    "            temp_band_range = band_ranges[i]\n",
    "\n",
    "            temp_row = []\n",
    "\n",
    "            temp_row.append(temp_band_range)\n",
    "            temp_row.append(temp_num_sig_exp)\n",
    "\n",
    "            for j in np.arange(len(temp_exp_list)):\n",
    "                temp_row.append(temp_exp_list[j])\n",
    "\n",
    "            wr.writerow(temp_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-episode",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Organize actual LFP data\n",
    "## And Output best actual LFP examples\n",
    "\n",
    "output_data = 1\n",
    "\n",
    "total_num_iter = 1\n",
    "num_bands = 7\n",
    "num_files = 43\n",
    "beg_ind = 0\n",
    "end_ind = 1200\n",
    "Tw = 240\n",
    "\n",
    "bs_sig_exp = np.zeros((total_num_iter,num_bands))\n",
    "bs_score_mat = np.empty((total_num_iter, num_files, (end_ind-beg_ind) - Tw, num_bands))\n",
    "bs_neuron_data_storage = np.zeros((total_num_iter, num_files, Tw, num_bands))\n",
    "bs_PO2_data_storage = np.zeros((total_num_iter, num_files, Tw, num_bands))\n",
    "bs_corr_mat = np.zeros((total_num_iter, num_files, num_bands))\n",
    "bs_pvalue_mat = np.zeros((total_num_iter, num_files, num_bands))\n",
    "\n",
    "for i in np.arange(7):\n",
    "        bs_sig_exp[:,i] = raw_bs_data_LFP[i][0].ravel()\n",
    "        bs_score_mat[:, :, :, i] = raw_bs_data_LFP[i][1]\n",
    "        bs_neuron_data_storage[:, :, :, i] = raw_bs_data_LFP[i][2][:,:,:,0]\n",
    "        bs_PO2_data_storage[:, :, :, i] = raw_bs_data_LFP[i][3][:,:,:,0]\n",
    "        bs_corr_mat[:, :, i] = raw_bs_data_LFP[i][4][:,:,0]\n",
    "        bs_pvalue_mat[:, :, i] = raw_bs_data_LFP[i][5][:,:,0]\n",
    "\n",
    "actual_exp_list = raw_bs_data_LFP[0][6]\n",
    "        \n",
    "num_examples = 10\n",
    "        \n",
    "best_corr_ind_mat = np.zeros((7,num_examples))\n",
    "best_LFP_examples = np.zeros((Tw, num_bands, num_examples))\n",
    "best_PO2_examples = np.zeros((Tw, num_bands, num_examples))\n",
    "\n",
    "for i in np.arange(num_bands):\n",
    "    \n",
    "    exp_corr_vec = bs_corr_mat[0,:,i]\n",
    "    \n",
    "    best_corr_inds = np.argsort(exp_corr_vec)\n",
    "    best_corr_inds = best_corr_inds[::-1]\n",
    "    \n",
    "    for k in np.arange(num_examples):\n",
    "        best_exp_ind = best_corr_inds[k]\n",
    "\n",
    "        best_corr_ind_mat[i, k] = best_exp_ind\n",
    "\n",
    "        best_LFP_examples[:, i, k] = bs_neuron_data_storage[0, best_exp_ind, :, i]\n",
    "        best_PO2_examples[:, i, k] = bs_PO2_data_storage[0, best_exp_ind, :, i]\n",
    "\n",
    "LFP_num = 3\n",
    "case_num = 5\n",
    "\n",
    "norm_best_LFP_examples = (best_LFP_examples - np.mean(best_LFP_examples, axis=0))/np.std(best_LFP_examples, axis=0)\n",
    "norm_best_PO2_examples = (best_PO2_examples - np.mean(best_PO2_examples, axis=0))/np.std(best_PO2_examples, axis=0)\n",
    "\n",
    "plt.plot(norm_best_LFP_examples[:,LFP_num, case_num])\n",
    "plt.plot(norm_best_PO2_examples[:,LFP_num, case_num])\n",
    "\n",
    "if output_data:\n",
    "\n",
    "    for k in np.arange(num_examples):\n",
    "\n",
    "        temp_PO2_examples = norm_best_PO2_examples[:,:,k]\n",
    "        temp_LFP_examples = norm_best_LFP_examples[:,:,k]\n",
    "        \n",
    "        output_folder = '[Output folder for best LFP examples]'\n",
    "\n",
    "        if not os.path.isdir(output_folder):\n",
    "            os.mkdir(output_folder)\n",
    "            \n",
    "        output_path = output_folder + 'LFP_actual_examples_case' + str(int(k+1)) + '.csv'    \n",
    "        \n",
    "        with open(output_path,'w') as result_file:\n",
    "            wr = csv.writer(result_file, dialect='excel')\n",
    "\n",
    "            num_obs = temp_PO2_examples.shape[0]\n",
    "            num_bands = temp_PO2_examples.shape[1]\n",
    "            \n",
    "            band_ranges = ['0-4Hz', '4-7Hz', '8-15Hz', '16-31Hz', '30-50Hz', '50-80Hz', '80-100Hz']\n",
    "            header1 = []\n",
    "            header2 = []\n",
    "            exp_header = []\n",
    "            tw_header = []\n",
    "            \n",
    "            for h in np.arange(num_bands):\n",
    "                if h == 0:\n",
    "                    header1.append(\"PO2\")\n",
    "                else:\n",
    "                    header1.append(\"\")\n",
    "                    \n",
    "                header2.append(band_ranges[h])\n",
    "                \n",
    "                best_exp_ind = int(best_corr_ind_mat[h, k])\n",
    "                \n",
    "                temp_exp = actual_exp_list[best_exp_ind]\n",
    "                exp_header.append(temp_exp)\n",
    "                \n",
    "                max_ind = np.argmax(bs_score_mat[0, best_exp_ind, :, h])\n",
    "                tw_string = str(max_ind/2) + \"s-\" + str((max_ind + 240)/2) + \"s\"\n",
    "                tw_header.append(tw_string)\n",
    "            \n",
    "            header1.append(\"\")\n",
    "            header2.append(\"\")\n",
    "            exp_header.append(\"\")\n",
    "            tw_header.append(\"\")\n",
    "            \n",
    "            for h in np.arange(num_bands):\n",
    "                if h == 0:\n",
    "                    header1.append(\"LFP\")\n",
    "                else:\n",
    "                    header1.append(\"\")\n",
    "                \n",
    "                header2.append(band_ranges[h])\n",
    "                \n",
    "                best_exp_ind = int(best_corr_ind_mat[h, k])\n",
    "                \n",
    "                temp_exp = actual_exp_list[best_exp_ind]\n",
    "                exp_header.append(temp_exp)\n",
    "                \n",
    "                max_ind = np.argmax(bs_score_mat[0, best_exp_ind, :, h])\n",
    "                tw_string = str(max_ind/2) + \"s-\" + str((max_ind + 240)/2) + \"s\"\n",
    "                tw_header.append(tw_string)\n",
    "            \n",
    "            wr.writerow(header1)\n",
    "            wr.writerow(header2)\n",
    "            wr.writerow(exp_header)\n",
    "            wr.writerow(tw_header)\n",
    "            \n",
    "            for i in np.arange(num_obs):\n",
    "                temp_list = []\n",
    "                \n",
    "                for j in np.arange(num_bands):\n",
    "                    temp_list.append(temp_PO2_examples[i,j])\n",
    "                \n",
    "                temp_list.append(\"\")\n",
    "                \n",
    "                for j in np.arange(num_bands):\n",
    "                    temp_list.append(temp_LFP_examples[i,j])\n",
    "                    \n",
    "                wr.writerow(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "married-blend",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Determine synchronization values/percentage on LFP data\n",
    "\n",
    "alpha = .05       # p-value significance level\n",
    "N = 100           # Number of surrogates to run for Synchronization Index analysis\n",
    "PI_thresh = .3    # Threshold of participation index in order to be added to participation cluster \n",
    "\n",
    "output_folder = '[Output folder for LFP synchronization data]'\n",
    "\n",
    "for i in np.arange(num_bands):\n",
    "    \n",
    "    sig_exp_list = raw_bs_data_LFP[i][8]\n",
    "    \n",
    "    temp_exp_list = []\n",
    "    sync_values = []\n",
    "    cluster_perc_list = []\n",
    "    max_tw_list = []\n",
    "    \n",
    "    for j in np.arange(num_files):\n",
    "        \n",
    "        exp_file = raw_bs_data_LFP[0][6][j]\n",
    "        \n",
    "        if exp_file in sig_exp_list:\n",
    "            \n",
    "            temp_exp_list.append(exp_file)\n",
    "            \n",
    "            # Determine time window of max R value\n",
    "            temp_score_vec = bs_score_mat[0,j,:,i]\n",
    "            max_tw_ind = np.argmax(temp_score_vec)\n",
    "            temp_beg_ind = max_tw_ind\n",
    "            temp_end_ind = max_tw_ind + Tw\n",
    "            \n",
    "            tw_string = str(temp_beg_ind/2) + \"s-\" + str(temp_end_ind/2) + \"s\"  \n",
    "            max_tw_list.append(tw_string)\n",
    "            \n",
    "            # Pull neuron data corresponding to the LFP experiment\n",
    "            cut_flname = exp_file[:-8]\n",
    "            smry_flname = cut_flname + \"_SummarySmoothed.csv\"\n",
    "            smry_data = np.genfromtxt('[Path to neuron data]'\n",
    "                          + smry_flname, delimiter = ',')\n",
    "            neuron_data = smry_data[:,3:]\n",
    "            \n",
    "            # Select appropriate time window for nueorn data\n",
    "            neuron_data = neuron_data[temp_beg_ind:temp_end_ind,:]\n",
    "        \n",
    "            # Initialize Bonferroni alpha sig level for CMA analysis\n",
    "            temp_num_neurons = neuron_data.shape[1]    \n",
    "            temp_bonf_alpha = .05/temp_num_neurons\n",
    "            \n",
    "            # Normalize time window neuron data and calculate eigvalues and eigvectors to run CMA analysis\n",
    "            X_norm_temp_orig = (neuron_data - np.mean(neuron_data, axis = 0))/np.std(neuron_data, axis = 0)\n",
    "            X_p = X_norm_temp_orig.T\n",
    "            cov_mat = np.cov(X_p)\n",
    "            eig_values, eig_vecs = np.linalg.eig(cov_mat)\n",
    "            sort_ind = np.argsort(eig_values)\n",
    "            sorted_eig_values = eig_values[sort_ind]\n",
    "            sorted_eig_vecs = eig_vecs[:,sort_ind]\n",
    "\n",
    "            # Initialized smoothing time window\n",
    "            smooth = [1,6]\n",
    "            \n",
    "            sync_ind_vec, PI_mat, _, _ = nvf.CMA_Sync(X_norm_temp_orig, N, sorted_eig_values, \n",
    "                                                      sorted_eig_vecs, temp_bonf_alpha, smooth)\n",
    "            \n",
    "            sync_ind_value = sync_ind_vec[-1]\n",
    "            sync_values.append(sync_ind_value)\n",
    "            \n",
    "            PI_vec = PI_mat[0,:]\n",
    "            perc_cluster = (np.sum(PI_vec > PI_thresh))/len(PI_vec)\n",
    "            cluster_perc_list.append(perc_cluster)\n",
    "            \n",
    "    temp_file_ext = \"Band\" + str(i+1) + \".csv\"\n",
    "    output_path = output_folder + temp_file_ext\n",
    "    \n",
    "    with open(output_path,'w') as result_file:\n",
    "        wr = csv.writer(result_file, dialect='excel')\n",
    "\n",
    "        header = [\"Experiment\", \"Synchronization Index\", \"Percent in Cluster\", \"Time Window\"]\n",
    "        wr.writerow(header)\n",
    "\n",
    "        for k in np.arange(len(temp_exp_list)):\n",
    "            int_k  = int(k)\n",
    "            temp_row = [temp_exp_list[int_k], sync_values[int_k], cluster_perc_list[int_k], max_tw_list[int_k]]\n",
    "            wr.writerow(temp_row)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-overhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize simulated LFP data into individual matrices\n",
    "total_num_iter = 100\n",
    "num_files = 43\n",
    "num_bands = 7\n",
    "\n",
    "beg_ind = 0\n",
    "end_ind = 1200\n",
    "Tw = 240\n",
    "\n",
    "bs_sig_exp = np.zeros((total_num_iter,num_bands))\n",
    "bs_score_mat = np.empty((total_num_iter, num_files, (end_ind-beg_ind) - Tw, num_bands))\n",
    "bs_neuron_data_storage = np.zeros((total_num_iter, num_files, Tw, num_bands))\n",
    "bs_PO2_data_storage = np.zeros((total_num_iter, num_files, Tw, num_bands))\n",
    "bs_corr_mat = np.zeros((total_num_iter, num_files, num_bands))\n",
    "bs_pvalue_mat = np.zeros((total_num_iter, num_files, num_bands))\n",
    "\n",
    "for i in np.arange(10):\n",
    "    column_ind = int(np.floor(i/2))\n",
    "    \n",
    "    bound_ind = 50\n",
    "    \n",
    "    if i % 2 == 0:\n",
    "        bs_sig_exp[:bound_ind,column_ind] = raw_bs_data_LFP[i][0].ravel()\n",
    "        bs_score_mat[:bound_ind, :, :, column_ind] = raw_bs_data_LFP[i][1]\n",
    "        bs_neuron_data_storage[:bound_ind, :, :, column_ind] = raw_bs_data_LFP[i][2][:,:,:,0]\n",
    "        bs_PO2_data_storage[:bound_ind, :, :, column_ind] = raw_bs_data_LFP[i][3][:,:,:,0]\n",
    "        bs_corr_mat[:bound_ind, :, column_ind] = raw_bs_data_LFP[i][4][:,:,0]\n",
    "        bs_pvalue_mat[:bound_ind, :, column_ind] = raw_bs_data_LFP[i][5][:,:,0]\n",
    "        \n",
    "    else:\n",
    "        bs_sig_exp[bound_ind:, column_ind] = raw_bs_data_LFP[i][0].ravel()\n",
    "        bs_score_mat[bound_ind:, :, :, column_ind] = raw_bs_data_LFP[i][1]\n",
    "        bs_neuron_data_storage[bound_ind:, :, :, column_ind] = raw_bs_data_LFP[i][2][:,:,:,0]\n",
    "        bs_PO2_data_storage[bound_ind:, :, :, column_ind] = raw_bs_data_LFP[i][3][:,:,:,0]\n",
    "        bs_corr_mat[bound_ind:, :, column_ind] = raw_bs_data_LFP[i][4][:,:,0]\n",
    "        bs_pvalue_mat[bound_ind:, :, column_ind] = raw_bs_data_LFP[i][5][:,:,0]        \n",
    "        \n",
    "bs_sig_exp[:, 5] = raw_bs_data_LFP[10][0].ravel()\n",
    "bs_score_mat[:, :, :, 5] = raw_bs_data_LFP[10][1]\n",
    "bs_neuron_data_storage[:, :, :, 5] = raw_bs_data_LFP[10][2][:,:,:,0]\n",
    "bs_PO2_data_storage[:, :, :, 5] = raw_bs_data_LFP[10][3][:,:,:,0]\n",
    "bs_corr_mat[:, :, 5] = raw_bs_data_LFP[10][4][:,:,0]\n",
    "bs_pvalue_mat[:, :, 5] = raw_bs_data_LFP[10][5][:,:,0]\n",
    "\n",
    "bs_sig_exp[:, 6] = raw_bs_data_LFP[11][0].ravel()\n",
    "bs_score_mat[:, :, :, 6] = raw_bs_data_LFP[11][1]\n",
    "bs_neuron_data_storage[:, :, :, 6] = raw_bs_data_LFP[11][2][:,:,:,0]\n",
    "bs_PO2_data_storage[:, :, :, 6] = raw_bs_data_LFP[11][3][:,:,:,0]\n",
    "bs_corr_mat[:, :, 6] = raw_bs_data_LFP[11][4][:,:,0]\n",
    "bs_pvalue_mat[:, :, 6] = raw_bs_data_LFP[11][5][:,:,0]\n",
    "\n",
    "print(bs_neuron_data_storage.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-contributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output best examples of LFP surrogate data\n",
    "\n",
    "output_data = 1\n",
    "\n",
    "bonf_alpha = .05/7\n",
    "\n",
    "num_examples = 10\n",
    "\n",
    "# Band_num, iter_ind, exp_ind\n",
    "max_corr_ind = np.zeros((num_bands, 2, num_examples))\n",
    "\n",
    "for z in np.arange(num_bands):\n",
    "\n",
    "    temp_max_corrs = np.zeros(total_num_iter)\n",
    "    temp_max_corrs_inds = np.zeros(total_num_iter)\n",
    "\n",
    "    for i in np.arange(total_num_iter):\n",
    "\n",
    "        exp_corr_list = bs_corr_mat[i,:,z]\n",
    "        actual_corr_list = np.zeros(len(exp_corr_list))\n",
    "\n",
    "        sig_corr_log = bs_pvalue_mat[i,:,z] < bonf_alpha \n",
    "        actual_corr_list[sig_corr_log] = exp_corr_list[sig_corr_log]\n",
    "\n",
    "        temp_max_corrs[i] = np.max(actual_corr_list)\n",
    "        temp_max_corrs_inds[i] = np.argmax(actual_corr_list)\n",
    "\n",
    "    best_corr_inds = np.argsort(temp_max_corrs)\n",
    "    best_corr_inds = best_corr_inds[::-1]\n",
    "\n",
    "    for k in np.arange(num_examples):\n",
    "    \n",
    "        best_ind = best_corr_inds[k]\n",
    "\n",
    "        max_corr_ind[z, 0, k] = best_ind\n",
    "        max_corr_ind[z, 1, k] = temp_max_corrs_inds[best_ind]\n",
    "    \n",
    "LFP_examples = np.zeros((Tw, num_bands, num_examples))\n",
    "PO2_examples = np.zeros((Tw, num_bands, num_examples))\n",
    "    \n",
    "for z in np.arange(num_bands):\n",
    "\n",
    "    print(z)\n",
    "    \n",
    "    for k in np.arange(num_examples):\n",
    "    \n",
    "        iter_id = int(max_corr_ind[z,0,k])\n",
    "        exp_id = int(max_corr_ind[z,1,k])\n",
    "\n",
    "        LFP_examples[:,z,k] = bs_neuron_data_storage[iter_id, exp_id, :, z]\n",
    "        PO2_examples[:,z,k] = bs_PO2_data_storage[iter_id, exp_id, :, z]\n",
    "\n",
    "LFP_examples_norm = (LFP_examples - np.mean(LFP_examples, axis=0))/np.std(LFP_examples, axis=0)\n",
    "PO2_examples_norm = (PO2_examples - np.mean(PO2_examples, axis=0))/np.std(PO2_examples, axis=0)\n",
    "\n",
    "exp_num = 0\n",
    "\n",
    "plt.plot(LFP_examples_norm[:,3,2])\n",
    "plt.plot(PO2_examples_norm[:,3,2])\n",
    "\n",
    "if output_data:\n",
    "\n",
    "    for k in np.arange(num_examples):\n",
    "\n",
    "        temp_PO2_examples = PO2_examples_norm[:,:,k]\n",
    "        temp_LFP_examples = LFP_examples_norm[:,:,k]\n",
    "        \n",
    "        output_path = '[Output path to LFP examples]/LFP_sim_examples_case' + str(int(k+1)) + '.csv'\n",
    "\n",
    "        with open(output_path,'w') as result_file:\n",
    "            wr = csv.writer(result_file, dialect='excel')\n",
    "\n",
    "            num_obs = temp_PO2_examples.shape[0]\n",
    "            num_bands = temp_PO2_examples.shape[1]\n",
    "            \n",
    "            band_ranges = ['0-4Hz', '4-7Hz', '8-15Hz', '16-31Hz', '30-50Hz', '50-80Hz', '80-100Hz']\n",
    "            header1 = []\n",
    "            header2 = []\n",
    "            \n",
    "            for h in np.arange(num_bands):\n",
    "                if h == 0:\n",
    "                    header1.append(\"PO2\")\n",
    "                else:\n",
    "                    header1.append(\"\")\n",
    "                    \n",
    "                header2.append(band_ranges[h])\n",
    "            \n",
    "            header1.append(\"\")\n",
    "            header2.append(\"\")\n",
    "            \n",
    "            for h in np.arange(num_bands):\n",
    "                if h == 0:\n",
    "                    header1.append(\"LFP\")\n",
    "                else:\n",
    "                    header1.append(\"\")\n",
    "                \n",
    "                header2.append(band_ranges[h])\n",
    "            \n",
    "            wr.writerow(header1)\n",
    "            wr.writerow(header2)\n",
    "            \n",
    "            for i in np.arange(num_obs):\n",
    "                temp_list = []\n",
    "                \n",
    "                for j in np.arange(num_bands):\n",
    "                    temp_list.append(temp_PO2_examples[i,j])\n",
    "                \n",
    "                temp_list.append(\"\")\n",
    "                \n",
    "                for j in np.arange(num_bands):\n",
    "                    temp_list.append(temp_LFP_examples[i,j])\n",
    "                    \n",
    "                wr.writerow(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-panic",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For each iteration, output the number of significantly Bonferroni correlated cases for each LFP band\n",
    "band_distr_sim = np.zeros((100,7))\n",
    "\n",
    "for i in np.arange(10):\n",
    "    column_ind = int(np.floor(i/2))\n",
    "    \n",
    "    if i % 2 == 0:\n",
    "        band_distr_sim[:50,column_ind] = raw_bs_data[i][0].ravel()\n",
    "    else:\n",
    "        band_distr_sim[50:,column_ind] = raw_bs_data[i][0].ravel()\n",
    "        \n",
    "band_distr_sim[:,5] = raw_bs_data[10][0].ravel()\n",
    "band_distr_sim[:,6] = raw_bs_data[11][0].ravel()\n",
    "\n",
    "band_ranges = ['0-4Hz', '4-7Hz', '8-15Hz', '16-31Hz', '30-50Hz', '50-80Hz', '80-100Hz']\n",
    "\n",
    "output_path = '[Output Path for LFP distribution data]/LFP_band_distr_sim_detrend.csv'\n",
    "\n",
    "import csv\n",
    "\n",
    "with open(output_path,'w') as result_file:\n",
    "    wr = csv.writer(result_file, dialect='excel')\n",
    "    \n",
    "    wr.writerow(band_ranges)\n",
    "    \n",
    "    for i in np.arange(band_distr_sim.shape[0]):\n",
    "        wr.writerow(band_distr_sim[i,:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
